{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e7329-c67b-433b-8d78-873d94b96b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "import re \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524f5ae-ab81-4b7d-95e1-5d664b41100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07466635-7830-4f9c-ae11-9dc06526cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab64338-df4d-41be-b75a-369ea7201a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deduplicated_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2011d-b022-483b-90b7-89550689f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_removed_df = deduplicated_df.dropna().reset_index(drop=True)\n",
    "cleaned_df = nulls_removed_df.copy()\n",
    "print(cleaned_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7155ad-1d09-43fe-9ac3-dc92a161e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f53abf-9fbf-4d69-b3db-296bd69c4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_df.drop(columns=['label'])\n",
    "y = cleaned_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f16f7-dccd-496c-bc5d-5448fa89c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3563e-7e5c-4ba0-8bbc-34bebe0bee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1cfbb-6dd2-449c-8c0e-36c052851041",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37dea5a-c673-409e-ac9c-640e15c524cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping 0 to 'Fake' and 1 to 'Real'\n",
    "label_mapping = {0: 'Fake', 1: 'Real'}\n",
    "\n",
    "# Plotting the pie chart with custom labels\n",
    "cleaned_df['label'].value_counts().rename(index=label_mapping).plot.pie(\n",
    "    autopct='%.2f%%',\n",
    "    labels=['Real', 'Fake']  # Explicitly set labels\n",
    ")\n",
    "\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e785a-5a57-4b74-be61-81f65e0827bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot for label distribution\n",
    "sns.countplot(data=cleaned_df, x='label', hue='label', legend=False, palette=['orange', 'blue'])\n",
    "plt.title('Label Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.xticks([0, 1], ['Fake', 'Real'])\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d517a27-10f5-4bce-b2c7-42a473a59160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donut plot for label distribution\n",
    "sizes = cleaned_df['label'].value_counts()\n",
    "colors = ['blue', 'orange']\n",
    "explode = (0.1, 0)  # explode the first slice\n",
    "\n",
    "plt.pie(sizes, labels=['Real', 'Fake'], autopct='%.2f%%', startangle=90, colors=colors, explode=explode)\n",
    "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "plt.title('Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf08ad-182e-47a0-9fdc-14542d5636db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab_size = Unique words in our Corpus (entire document)\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c164b9-4d98-4184-8a74-51c3ac6cba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = X.copy()\n",
    "\n",
    "# Reset the index to avoid errors caused by using dropna() earlier\n",
    "messages.reset_index(inplace=True)\n",
    "print(messages['title'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b03d7-8739-4f2c-8e27-8db4467a45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Preprocessing ###\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['title'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "# Create an instance of the CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.95, min_df=5, stop_words='english')\n",
    "dtm = cv.fit_transform(corpus)\n",
    "print(dtm.shape)\n",
    "\n",
    "\n",
    "# Inspect the corpus\n",
    "print(corpus[:30])\n",
    "\n",
    "# Calculate the maximum sentence length\n",
    "max_length = max(len(sentence.split()) for sentence in corpus)\n",
    "print(\"Maximum sentence length:\", max_length)\n",
    "\n",
    "# Convert Text to One Hot Representation and Add Padding\n",
    "onehot_repr = [one_hot(words, vocab_size) for words in corpus]\n",
    "print(onehot_repr[:5])\n",
    "\n",
    "sentence_length = 50\n",
    "embedded_docs = pad_sequences(onehot_repr, padding='pre', maxlen=sentence_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31bbff-79f8-4d6e-a2a6-f7a76582ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LDA Integration ###\n",
    "count_vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "dtm = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Print the top 15 words for each topic\n",
    "for index, topic in enumerate(lda.components_):  \n",
    "    print(f'The Top 15 Words For Topic #{index + 1}') \n",
    "    print([count_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-15:]])  \n",
    "    print('\\n')\n",
    "    \n",
    "# Transform our DTM \n",
    "topic_results = lda.transform(dtm)\n",
    "\n",
    # Adding Topic Labels
topic_labels = {
    1: 'Entertainment',
    2: 'Sports',
    3: 'Business',
    4: 'Politics',
    5: 'Technology'
    "\n",
    "def add_topic_labels(df, topic_results, topic_labels):\n",
    "    df['topic'] = topic_results.argmax(axis=1) + 1\n",
    "    df['topic_label'] = df['topic'].map(topic_labels)\n",
    "    \n",
    "add_topic_labels(df, topic_results, topic_labels)\n",
    "\n",
    "# Display the first 20 rows of the updated DataFrame \n",
    "print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d79d9-8266-443b-885f-cf9372960530",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VADER Sentiment Analysis ###\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiments = [analyzer.polarity_scores(title)['compound'] for title in corpus]\n",
    "\n",
    "X_sentiment = np.array(sentiments).reshape(-1, 1)\n",
    "X_final_with_topics = np.concatenate((embedded_docs, topic_results), axis=1)\n",
    "X_final_with_sentiment = np.concatenate((X_final_with_topics, X_sentiment), axis=1)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_final = np.array(X_final_with_sentiment)\n",
    "y_final = np.array(y)\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e527e-4985-4dc4-85f0-450f1666b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train LSTM models\n",
    "embedding_vector_features = 100\n",
    "\n",
    "# First Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_features))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n",
    "\n",
    "# Evaluate First Model\n",
    "y_log = model.predict(X_test)\n",
    "y_pred = np.where(y_log > 0.5, 1, 0)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print(acc)\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d1dbe-811d-416e-a910-18287fa15a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Model with Multiple LSTM Layers\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, embedding_vector_features))\n",
    "model1.add(LSTM(100, return_sequences=True))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(LSTM(50, return_sequences=True))\n",
    "model1.add(Dropout(0.1))\n",
    "model1.add(LSTM(20))\n",
    "model1.add(Dropout(0.1))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())\n",
    "\n",
    "model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n",
    "\n",
    "# Evaluate Second Model\n",
    "y_log_1 = model1.predict(X_test)\n",
    "y_pred_1 = np.where(y_log_1 > 0.5, 1, 0)\n",
    "acc_1 = accuracy_score(y_test, y_pred_1)\n",
    "confusion_mat_1 = confusion_matrix(y_test, y_pred_1)\n",
    "print(acc_1)\n",
    "print(confusion_mat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cab419-36f1-4750-b96c-4c5020218b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Model with Bidirectional LSTM\n",
    "embedding_vector_features = 150\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, embedding_vector_features))\n",
    "model2.add(Bidirectional(LSTM(200)))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "\n",
    "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=120)\n",
    "\n",
    "y_log_2 = model2.predict(X_test)\n",
    "y_pred_2 = np.where(y_log_2 > 0.5, 1, 0)\n",
    "acc_2 = accuracy_score(y_test, y_pred_2)\n",
    "confusion_mat_2 = confusion_matrix(y_test, y_pred_2)\n",
    "print(acc_2)\n",
    "print(confusion_mat_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
